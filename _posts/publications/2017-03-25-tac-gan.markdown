---
layout: project
title:  "TAC-GAN - Text Conditioned Auxiliary Classifier Generative Adversarial Network"
date:   2017-03-25 10:07:58
author: Ayushman Dash, John Cristian Borges Gamboa, Sheraz Ahmed, Muhammad Zeshan Afzal, Marcus Liwicki
categories:
- publication
- image understanding
- image generation
- generative adversarial neural networks
img: tac-gan.png
thumb: tac-gan.png
tagged: generative adversarial neural networks, image understanding, image generation, AlexNet, GoogLeNet
client:
website: https://arxiv.org/abs/1703.06412
type: preprint
---
In this work, we present the Text Conditioned Auxiliary Classifier Generative Adversarial Network, (TAC-GAN) a text to image Generative Adversarial Network (GAN) for synthesizing images from their text descriptions. Former approaches have tried to condition the generative process on the textual data; but allying it to the usage of class information, known to diversify the generated samples and improve their structural coherence, has not been explored. We trained the presented TAC-GAN model on the Oxford-102 dataset of flowers, and evaluated the discriminability of the generated images with Inception-Score, as well as their diversity using the Multi-Scale Structural Similarity Index (MS-SSIM). Our approach outperforms the state-of-the-art models, i.e., its inception score is 3.45, corresponding to a relative increase of 7.8% compared to the recently introduced StackGan. A comparison of the mean MS-SSIM scores of the training and generated samples per class shows that our approach is able to generate highly diverse images with an average MS-SSIM of 0.14 over all generated classes.